{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Parallelize Pandas with Dask.dataframe\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import dask\n",
        "from dask.distributed import Client, progress\n",
        "from dask import delayed\n",
        "df = None\n",
        "c = Client('tcp://localhost:8786')\n",
        "c.restart()\n",
        "c"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Run\n",
        "import os\n",
        "run = Run.get_context()\n",
        "ws = run.experiment.workspace\n",
        "\n",
        "## or load directly through blob file system\n",
        "# using https://github.com/dask/adlfs -- still pretty beta, \n",
        "# throws an error message, but seesm to work\n",
        "ds = ws.get_default_datastore()\n",
        "ACCOUNT_NAME = ds.account_name\n",
        "ACCOUNT_KEY = ds.account_key\n",
        "CONTAINER = ds.container_name\n",
        "print(CONTAINER)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.dataframe as dd\n",
        "from fsspec.registry import known_implementations\n",
        "known_implementations['abfs'] = {'class': 'adlfs.AzureBlobFileSystem'}\n",
        "STORAGE_OPTIONS={'account_name': ACCOUNT_NAME, 'account_key': ACCOUNT_KEY}\n",
        "df = dd.read_csv(f'abfs://{CONTAINER}/nyctaxig/sample.csv', \n",
        "                 storage_options=STORAGE_OPTIONS,\n",
        "                 parse_dates=['lpepPickupDatetime', 'lpepDropoffDatetime'], dtype={'tripType': 'float64'})"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# enable this code path instead of the above if you run into\n",
        "# any issues with the AzureBlobFileSystem (https://github.com/dask/adlfs)\n",
        "# this will load the data from the workspace blob storage mounted via blobFUSE\n",
        "if False:\n",
        "    from azureml.core import Workspace\n",
        "    ## get the last run on the dask experiment which should be running \n",
        "    ## our dask cluster, and retrieve the data path from it\n",
        "    ws = Workspace.from_config()\n",
        "    exp = ws.experiments['dask']\n",
        "    run = None\n",
        "    for run in ws.experiments['dask'].get_runs():\n",
        "        if run.get_status() == \"Running\":\n",
        "            cluster_run = run\n",
        "            break;\n",
        "\n",
        "    if (run == None):\n",
        "        raise Exception('Cluster should be in state \\'Running\\'')\n",
        "\n",
        "    data_path = cluster_run.get_metrics()['datastore'] + '/nyctaxig'\n",
        "\n",
        "\n",
        "    import dask\n",
        "    import dask.dataframe as dd\n",
        "    from dask import delayed\n",
        "\n",
        "    def load_data(path):\n",
        "        return dd.read_csv(path, parse_dates=['lpepPickupDatetime', 'lpepDropoffDatetime'])\n",
        "\n",
        "    data_2015 = data_path + '/2015'\n",
        "    data_2015_csv = data_2015 + '/*.csv'\n",
        "    df = delayed(load_data)(data_2015_csv).compute()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# fall back to this path if neither of the above paths have been enabled\n",
        "if df is None:\n",
        "    ## or in this case straight from GOOGLE Storage\n",
        "    import dask.dataframe as dd\n",
        "    df = dd.read_csv('gcs://anaconda-public-data/nyc-taxi/csv/2015/yellow_*.csv',\n",
        "                     storage_options={'token': 'anon'}, \n",
        "                     parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%time len(df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df.partitions"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%time df.map_partitions(len).compute().sum()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Dask DataFrames\n",
        "---------------\n",
        "\n",
        "*  Coordinate many Pandas DataFrames across a cluster\n",
        "*  Faithfully implement a subset of the Pandas API\n",
        "*  Use Pandas under the hood (for speed and maturity)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list of column names that need to be re-mapped\n",
        "remap = {}\n",
        "#remap['tpep_pickup_datetime'] = 'pickup_datetime'\n",
        "#remap['tpep_dropoff_datetime'] = 'dropoff_datetime'\n",
        "#remap['RatecodeID'] = 'rate_code'\n",
        "\n",
        "remap['lpepPickupDatetime'] = 'pickupDatetime'\n",
        "remap['lpepDropoffDatetime'] = 'dropoffDatetime'\n",
        "\n",
        "\n",
        "#create a list of columns & dtypes the df must have\n",
        "must_haves = {\n",
        "    'vendorID': 'object',\n",
        "    'pickupDatetime': 'datetime64[ms]',\n",
        "    'dropoffDatetime': 'datetime64[ms]',\n",
        "    'passengerCount': 'int32',\n",
        "    'tripDistance': 'float32',\n",
        "    'pickupLongitude': 'float32',\n",
        "    'pickupLatitude': 'float32',\n",
        "    'rateCodeID': 'int32',\n",
        "    'paymentType': 'int32',\n",
        "    'dropoffLongitude': 'float32',\n",
        "    'dropoffLatitude': 'float32',\n",
        "    'fareAmount': 'float32',\n",
        "    'tipAmount': 'float32',\n",
        "    'totalAmount': 'float32'\n",
        "}\n",
        "\n",
        "query_frags = [\n",
        "    'fareAmount > 0 and fareAmount < 500',\n",
        "    'passengerCount > 0 and passengerCount < 6',\n",
        "    'pickupLongitude > -75 and pickupLongitude < -73',\n",
        "    'dropoffLongitude > -75 and dropoffLongitude < -73',\n",
        "    'pickupLatitude > 40 and pickupLatitude < 42',\n",
        "    'dropoffLatitude > 40 and dropoffLatitude < 42'\n",
        "]\n",
        "query = ' and '.join(query_frags)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(frac=0.00001).compute()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function which takes a DataFrame partition\n",
        "def clean(df_part, remap, must_haves, query):    \n",
        "    df_part = df_part.query(query)\n",
        "    \n",
        "    # some col-names include pre-pended spaces remove & lowercase column names\n",
        "    # tmp = {col:col.strip().lower() for col in list(df_part.columns)}\n",
        "\n",
        "    # rename using the supplied mapping\n",
        "    df_part = df_part.rename(columns=remap)\n",
        "    \n",
        "    # iterate through columns in this df partition\n",
        "    for col in df_part.columns:\n",
        "        # drop anything not in our expected list\n",
        "        if col not in must_haves:\n",
        "            df_part = df_part.drop(col, axis=1)\n",
        "            continue\n",
        "\n",
        "        if df_part[col].dtype == 'object' and col in ['pickupDatetime', 'dropoffDatetime']:\n",
        "            df_part[col] = df_part[col].astype('datetime64[ms]')\n",
        "            continue\n",
        "            \n",
        "        # if column was read as a string, recast as float\n",
        "        if df_part[col].dtype == 'object':\n",
        "            df_part[col] = df_part[col].str.fillna('-1')\n",
        "            df_part[col] = df_part[col].astype('float32')\n",
        "        else:\n",
        "            # save some memory by using 32 bit floats\n",
        "            if 'int' in str(df_part[col].dtype):\n",
        "                df_part[col] = df_part[col].astype('int32')\n",
        "            if 'float' in str(df_part[col].dtype):\n",
        "                df_part[col] = df_part[col].astype('float32')\n",
        "            df_part[col] = df_part[col].fillna(-1)\n",
        "    \n",
        "    return df_part"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "taxi_df = clean(df, remap, must_haves, query)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from math import  pi\n",
        "from dask.array import cos, sin, arcsin, sqrt, floor\n",
        "import numpy as np\n",
        "\n",
        "def haversine_distance(pickupLatitude, pickupLongitude, dropoffLatitude, dropoffLongitude):\n",
        "    x_1 = pi / 180 * pickupLatitude\n",
        "    y_1 = pi / 180 * pickupLongitude\n",
        "    x_2 = pi / 180 * dropoffLatitude\n",
        "    y_2 = pi / 180 * dropoffLongitude\n",
        "\n",
        "    dlon = y_2 - y_1\n",
        "    dlat = x_2 - x_1\n",
        "    a = sin(dlat / 2)**2 + cos(x_1) * cos(x_2) * sin(dlon / 2)**2\n",
        "\n",
        "    c = 2 * arcsin(sqrt(a)) \n",
        "    r = 6371 # Radius of earth in kilometers\n",
        "\n",
        "    return c * r\n",
        "\n",
        "def day_of_the_week(day, month, year):\n",
        "    if month < 3:\n",
        "        shift = month\n",
        "    else:\n",
        "        shift = 0\n",
        "    Y = year - (month < 3)\n",
        "    y = Y - 2000\n",
        "    c = 20\n",
        "    d = day\n",
        "    m = month + shift + 1\n",
        "    return (d + floor(m * 2.6) + y + (y // 4) + (c // 4) - 2 * c) % 7\n",
        "        \n",
        "def add_features(df):\n",
        "    df['hour'] = df['pickupDatetime'].dt.hour.astype('int32')\n",
        "    df['year'] = df['pickupDatetime'].dt.year.astype('int32')\n",
        "    df['month'] = df['pickupDatetime'].dt.month.astype('int32')\n",
        "    df['day'] = df['pickupDatetime'].dt.day.astype('int32')\n",
        "    df['day_of_week'] = df['pickupDatetime'].dt.weekday.astype('int32')\n",
        "       \n",
        "    #df['diff'] = df['dropoff_datetime'].astype('int32') - df['pickup_datetime'].astype('int32')\n",
        "    df['diff'] = df['dropoffDatetime'] - df['pickupDatetime']\n",
        "    \n",
        "    df['pickupLatitude_r'] = (df['pickupLatitude'] // .01 * .01).astype('float32')\n",
        "    df['pickupLongitude_r'] = (df['pickupLongitude'] // .01 * .01).astype('float32')\n",
        "    df['dropoffLatitude_r'] = (df['dropoffLatitude'] // .01 * .01).astype('float32')\n",
        "    df['dropoffLongitude_r'] = (df['dropoffLongitude'] // .01 * .01).astype('float32')\n",
        "    \n",
        "    #df = df.drop('pickup_datetime', axis=1)\n",
        "    #df = df.drop('dropoff_datetime', axis=1)\n",
        "\n",
        "    #df = df.apply_rows(haversine_distance_kernel,\n",
        "    #                   incols=['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'],\n",
        "    #                   outcols=dict(h_distance=np.float32),\n",
        "    #                   kwargs=dict())\n",
        "\n",
        "    import numpy\n",
        "\n",
        "    df['h_distance'] = haversine_distance(df['pickupLatitude'], \n",
        "                                          df['pickupLongitude'], \n",
        "                                          df['dropoffLatitude'], \n",
        "                                          df['dropoffLongitude']).astype('float32')\n",
        "\n",
        "    #df = df.apply_rows(day_of_the_week_kernel,\n",
        "    #                   incols=['day', 'month', 'year'],\n",
        "    #                   outcols=dict(day_of_week=np.float32),\n",
        "    #                   kwargs=dict())\n",
        "    #df['day_of_week'] = numpy.empty(len(df), dtype=np.int32)\n",
        "    #day_of_the_week_kernel(df['day'],\n",
        "    #                       df['month'],\n",
        "    #                       df['year'],\n",
        "    #                       df['day_of_week'])\n",
        "    \n",
        "    \n",
        "    df['is_weekend'] = (df['day_of_week']>5).astype(\"int32\")\n",
        "    return df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "taxi_df = add_features(taxi_df)\n",
        "taxi_df.dtypes"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%time len(taxi_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "taxi_df = taxi_df.persist()\n",
        "progress(taxi_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%time taxi_df.passengerCount.sum().compute()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute average trip distance grouped by passenger count\n",
        "taxi_df.groupby('passengerCount').tripDistance.mean().compute()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tip Fraction, grouped by day-of-week and hour-of-day"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = taxi_df[(taxi_df.tipAmount > 0) & (taxi_df.fareAmount > 0)]\n",
        "df2['tipFraction'] = df2.tipAmount / df2.fareAmount"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Group df.tpep_pickup_datetime by dayofweek and hour\n",
        "dayofweek = df2.groupby(df2.pickupDatetime.dt.dayofweek).tipFraction.mean() \n",
        "hour = df2.groupby(df2.pickupDatetime.dt.hour).tipFraction.mean()\n",
        "\n",
        "dayofweek, hour = dask.persist(dayofweek, hour)\n",
        "progress(dayofweek, hour)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot results\n",
        "\n",
        "This requires matplotlib to be installed"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "hour.compute().plot(figsize=(10, 6), title='Tip Fraction by Hour')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dayofweek.compute().plot(figsize=(10, 6), title='Tip Fraction by Day of Week')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "taxi_df.groupby('passengerCount').fareAmount.mean().compute().sort_index().plot(legend=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "taxi_df.groupby(taxi_df.passengerCount).tripDistance.mean().compute().plot(legend=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "by_payment = taxi_df.groupby(taxi_df.paymentType).fareAmount.count().compute()\n",
        "by_payment.index = by_payment.index.map({1: 'Credit card',\n",
        "    2: 'Cash',\n",
        "    3: 'No charge',\n",
        "    4: 'Dispute',\n",
        "    5: 'Unknown',\n",
        "    6: 'Voided trip'})"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "by_payment.plot(legend=True, kind='bar')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's save the transformed dataset back to blob"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "output_uuid = uuid.uuid1().hex\n",
        "run.log('output_uuid', output_uuid)\n",
        "\n",
        "output_path = run.get_metrics()['datastore'] + '/output/' + output_uuid + '.parquet'\n",
        "\n",
        "print('save parquet to ', output_path)\n",
        "\n",
        "taxi_df.to_parquet(output_path)\n",
        "\n",
        "print('done')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import dask\n",
        "import dask.dataframe as dd\n",
        "\n",
        "df = dd.read_parquet(output_path)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}